{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b957cb",
   "metadata": {},
   "source": [
    "# Building Deep Network using Grid Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc4216",
   "metadata": {},
   "source": [
    "## Import necessary libraries and functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from time import time\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "# to suppress the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa47e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random variables to get the same results on different devices\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b951a8",
   "metadata": {},
   "source": [
    "### Define additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, main_title=None):\n",
    "    \"\"\"\n",
    "    Plot training & validation accuracy from history.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if main_title:\n",
    "        plt.suptitle(main_title, fontsize=16, y=1.05)\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(c_matrix, labels):\n",
    "    \"\"\"\n",
    "    Print confusion matrix with sns.heatmap\n",
    "    \"\"\"\n",
    "    ax = sns.heatmap(c_matrix, annot=True, linewidth=1.5, fmt='d',\n",
    "                     xticklabels=labels,\n",
    "                     yticklabels=labels,\n",
    "                     cbar=False, cmap='Blues')\n",
    "\n",
    "    ax.set_xlabel(\"Prediction\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_confusion_matrix_alt(c_matrix, labels):\n",
    "    \"\"\"\n",
    "    sns.heatmap didn't show all values, so this is malual heatmap,\n",
    "    that works properly.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(c_matrix, cmap='Blues')\n",
    "    # Add annotations\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            if i==j:\n",
    "                color='white'\n",
    "            else:\n",
    "                color='black'\n",
    "            ax.text(j, i, c_matrix[i, j], va='center', ha='center', color=color, fontsize=12)\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff6fc2",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "df = pd.read_csv('/Users/jts/Documents/college/Programming/CA2/glass_data.csv')\n",
    "df = shuffle(df, random_state=seed)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e52ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X_features = df.drop('type', axis=1)\n",
    "y_tagret = df['type']\n",
    "print('Data shapes:', X_features.shape, y_tagret.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3e38f",
   "metadata": {},
   "source": [
    "## Testing the model architectures and hyperparameters\n",
    "The hyperparameters that we're going to test:\n",
    "- units in Dense layer,\n",
    "- optimizers,\n",
    "- batch size,\n",
    "- split size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c102bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_units=64, optimizer='adam', input_dim=9):\n",
    "    # Create basic architecture\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Dense(hidden_units, input_dim=input_dim, activation='relu'),\n",
    "        #Hidden layer\n",
    "        Dense(hidden_units, activation='relu'),\n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def perform_grid_search(X, y, param_grid, export_csv: bool =True):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param X: Independent variables.\n",
    "    :param y: Dependent variable.\n",
    "    :param param_grid: Parameters to test.\n",
    "    :param export_csv: Save full report of each Default is True.\n",
    "    \n",
    "    :return  results: A list of dictionaries that includes best models found by GridSearchCV\n",
    "    \"\"\"\n",
    "    # Fix randomness in results\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # To store full results and create CSV afterwords\n",
    "    main_df = pd.DataFrame()\n",
    "    # Create new variable to iterate over and delete it for param_grid dict\n",
    "    test_sizes = param_grid['test_size']\n",
    "    del param_grid['test_size']\n",
    "    \n",
    "    results = []\n",
    "    for test_size in test_sizes:\n",
    "        # Create a KerasClassifier\n",
    "        model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "        # Create a pipeline with a StandardScaler and our model\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model) ])\n",
    "        # Set Early stopping so model won't overfit\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        print('* Start testing of', test_size, 'test size. *')\n",
    "        # Split your data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "\n",
    "        # Use GridSearchCV to search through the hyperparameter grid\n",
    "        grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='accuracy', cv=3, verbose=0)\n",
    "        grid_result = grid.fit(X_train, y_train, model__callbacks=[early_stopping])\n",
    "\n",
    "        # Store results in a DataFrame\n",
    "        results_df = pd.DataFrame(grid_result.cv_results_)\n",
    "        # Add test_size as col to results of iteration\n",
    "        results_df['test_size'] = [test_size] * len(results_df)\n",
    "\n",
    "        # Get the best hyperparameters\n",
    "        best_params = grid_result.best_params_\n",
    "\n",
    "        # Create a new model with the best hyperparameters\n",
    "        arch_param = {k.replace('model__', ''): v \n",
    "                      for k,v in best_params.items() \n",
    "                      if k!='model__batch_size'}\n",
    "        best_model = create_model(**arch_param)\n",
    "\n",
    "        # Print the best hyperparameters and corresponding accuracy\n",
    "        print(\"Best score: %f using %s\" % (grid_result.best_score_, test_size))\n",
    "        print(best_params)\n",
    "\n",
    "        # Predict on the test set with the best model\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the accuracy on the test set\n",
    "        y_pred = np.where(y_pred.flatten().round(2) > 0.5, 1, 0)\n",
    "        accuracy = accuracy_score(y_test.to_numpy(), y_pred)\n",
    "        print(\"Test Accuracy: %.2f%%\" % (accuracy * 100))\n",
    "        pd.concat([main_df, results_df])\n",
    "        results.append({\n",
    "            'test_size': test_size,\n",
    "            'best_param': best_params,\n",
    "            'accuracy': accuracy,})\n",
    "        print('* Finished testing', test_size, 'test size. *')\n",
    "    # Export results to CSV\n",
    "    if export_csv:\n",
    "        main_df.to_csv('grid_search_results.csv', index=False)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c076c71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'model__hidden_units': [4, 16, 512],\n",
    "    'model__optimizer': [tf.keras.optimizers.legacy.Adam(),\n",
    "                         tf.keras.optimizers.legacy.SGD(momentum=0.9),\n",
    "                        # tf.keras.optimizers.legacy.RMSprop()\n",
    "                        ],\n",
    "    'model__batch_size': [100, 50, 10],\n",
    "    'test_size': [.1, .2, .3]\n",
    "}\n",
    "# Because GridSearchCV has a pipeline with StandardScaler\n",
    "# we're passing not scaled X and Y\n",
    "gs_t1 = time()\n",
    "# We don't want to show all data, so we take only first 30,000 rows from dataset.\n",
    "grid_search_results = perform_grid_search(X_features, y_tagret, param_grid)\n",
    "gs_t2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb249abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished in {round((gs_t2-gs_t1)/60)} min.')\n",
    "# Best models in each value of test split size\n",
    "grid_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51200a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the results\n",
    "test_sizes = [entry['test_size'] for entry in grid_search_results]\n",
    "accuracies = [entry['accuracy']*100 for entry in grid_search_results]\n",
    "\n",
    "plt.plot(test_sizes, accuracies, marker='.', linestyle='-')\n",
    "plt.title('GridSearch results:\\nTest Size vs Accuracy', y=1.01)\n",
    "plt.xlabel('Test Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(round(min(accuracies))-5, round(max(accuracies))+5)\n",
    "# Add values of each point \n",
    "for x, y in zip(test_sizes, accuracies):\n",
    "    plt.text(x, y, f'{y:.2f}', ha='left', va='bottom', color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee494df9",
   "metadata": {},
   "source": [
    "## Choosing parameters and training the model\n",
    "Based on received results, let's choose the most accurate model to proceed with our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the most optimal model with max accuracy\n",
    "best_model_params = max(grid_search_results, key=lambda x: x['accuracy'])\n",
    "# Print parameters of the most optimal model\n",
    "print('The most optimal model parameters\\n\\tTest size:', best_model_params['test_size'])\n",
    "for k, v in best_model_params['best_param'].items():\n",
    "    k = k.replace('model__', '').replace('_', \" \")\n",
    "    k = k[0].upper() + k[1:]\n",
    "    if k == 'Optimizer':\n",
    "        print(f'\\t{k}: {v.__class__.__name__}')\n",
    "    else:\n",
    "        print(f'\\t{k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b5356",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets with found test_size\n",
    "real_size = round(X_features.shape[0]/30000)*best_model_params['test_size']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_tagret, \n",
    "                                                    test_size=real_size,\n",
    "                                                    random_state=seed)\n",
    "\n",
    "# Scaling of features\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c0bc8",
   "metadata": {},
   "source": [
    "### Define the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c979f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tune_lr(X_train, y_train, epochs=50):\n",
    "    model_tune = Sequential()\n",
    "    model_tune.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model_tune.add(Dense(64, activation='relu'))\n",
    "    model_tune.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    lr_schedule = LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.legacy.SGD(momentum=0.9) # for M2 processor only\n",
    "    # optimizer = tf.keras.optimizers.Adam()  \n",
    "    \n",
    "    model_tune.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    history = model_tune.fit(X_train, y_train, epochs=epochs, callbacks=[lr_schedule], verbose=0)\n",
    "    lrs = 1e-8 * (10 ** (np.arange(epochs) / 20))\n",
    "    # Plot the learning rate graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.grid(True)\n",
    "    plt.semilogx(lrs, history.history[\"loss\"])\n",
    "    plt.axis([min(lrs), max(lrs), min(history.history[\"loss\"]), max(history.history[\"loss\"])])\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('binary_crossentropy')\n",
    "    plt.tick_params('both', length=10, width=1, which='both')\n",
    "    plt.show()\n",
    "\n",
    "    optimal_lr_index = np.argmin(history.history[\"loss\"])\n",
    "    optimal_lr = lrs[optimal_lr_index]\n",
    "\n",
    "    print(f'Optimal Learning Rate: {optimal_lr}')\n",
    "    return optimal_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find learning rate\n",
    "optimal_lr = tune_lr(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d299a587",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "Build, train and evaluate the model with chosen hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdbf41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = best_model_params['best_param']['model__hidden_units']\n",
    "batch_size = best_model_params['best_param']['model__batch_size']\n",
    "# Get the class of the best optimizer\n",
    "optimizer_class = best_model_params['best_param']['model__optimizer'].__class__\n",
    "# Create a new optimizer instance with the same configuration\n",
    "optimizer = optimizer_class()\n",
    "optimizer.learning_rate.assign(optimal_lr)\n",
    "\n",
    "# We are increasing no of epochs,\n",
    "# because we added learning_rate to optimizer\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c300d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "        # Input layer\n",
    "        Dense(hidden_units, input_dim=X_train.shape[1], activation='relu'),\n",
    "        #Hidden layer\n",
    "        Dense(hidden_units, activation='relu'),\n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd2254e",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61026e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare own callback class to stop training when conditions met \n",
    "class val_metrics_stop(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        To stop the training when the val_loss or val_accuracy meets conditions.\n",
    "        \"\"\"\n",
    "        self.stopped_epoch = 0\n",
    "        if self.patience < epoch and (logs.get('val_loss') < 0.01 or logs.get('val_accuracy') > 0.9999):\n",
    "                # Stop if threshold is met\n",
    "                print(\"\\nCancelling training!\")\n",
    "                self.model.stop_training = True\n",
    "                self.stopped_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d2f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EarlyStopping will stop training\n",
    "# when 'val_loss' is the same for 5 epochs\n",
    "val_loss_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "val_accuracy_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1)\n",
    "# Instantiate own callback class\n",
    "val_metrics_stop_numbers = val_metrics_stop(patience=10)\n",
    "\n",
    "tm1 = time()\n",
    "model_history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=epochs, batch_size=batch_size,\n",
    "                    callbacks=[val_metrics_stop_numbers, val_loss_stop, val_accuracy_stop],)\n",
    "tm2 = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a17df0",
   "metadata": {},
   "source": [
    "### Evaluation of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_history(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73706c67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculating and plotting confusion_matrix\n",
    "y_test_pred = model.predict(X_test, verbose=0)\n",
    "y_test_pred_new = np.where(y_test_pred.flatten().round(2) > 0.5, 1, 0)\n",
    "c_matrix = confusion_matrix(y_test, y_test_pred_new)\n",
    "labels = ['Not CAFV', 'CAFV']\n",
    "\n",
    "plot_confusion_matrix(c_matrix, labels)\n",
    "plot_confusion_matrix_alt(c_matrix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f246b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No of epochs\n",
    "epochs_list = list({val_metrics_stop_numbers.stopped_epoch, val_accuracy_stop.stopped_epoch, val_loss_stop.stopped_epoch, epochs})\n",
    "epochs_list.sort()\n",
    "epochs_trained = min(epochs_list[1:])\n",
    "print('Trained on', epochs_trained, 'epochs')\n",
    "\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "y_pred = np.where(y_pred.flatten().round(2) > 0.5, 1, 0)\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "scores = model.evaluate(X_train, y_train, verbose=0,)\n",
    "print(f\"Training Accuracy: {round(scores[1]*100, 4)}%\")\n",
    "scores = model.evaluate(X_test, y_test, verbose=0,)\n",
    "tes_a_1 = scores[1]*100\n",
    "print(f\"Testing Accuracy: {round(scores[1]*100, 4)}%\")\n",
    "print(f\"Precision: {precision:.4f}\") # fraction of positive predictions which are correct\n",
    "print(f\"Recall: {recall:.4f}\") # True positive rate \n",
    "print('Time spent on training:', round(tm2-tm1), 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f0460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
